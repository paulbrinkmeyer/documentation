Apache Spark interfaces:
- Resilient Distributed Dataset (RDD)
    * available languages: Java, Python & Scala
- DataFrame
    * Superset of RDD
    * available languages: Java, Python & Scala
- DataSet
    * Combo of RDD & DataFrame
    * available languages: Java & Scala (no Python)


Spark commands:
data = spark.read.csv(
    "/databricks-datasets/samples/population-vs-price/data_geo.csv" # location 
    header="true",
    inferSchema="true"
)                          # read data from a csv file

data.cache()                              # Cache data for faster reuse
data = data.dropna()                      # drop rows with missing values
data.take(x)                              # top x values of a table
display(data)                             # show all the data
data.createOrReplaceTempView("data_geo")  # creats a table called data_geo that can be queried by SQL now


Python:
--------------------------------------------------------------------------------------------------------------
dbutils.fs.ls("/databricks-datasets/samples/docs/")                        # how to read a file location
textFile = spark.read.text("/databricks-datasets/samples/docs/README.md")  # how to load a file
textFile.count()                                                           # count the number lines in the file
textFile.first()                                                           # get the first line of the file
linesWithSpark = textFile.filter(textFile.value.contains("Spark"))         # filter the file with lines that have "Spark" in it
linesWithSpark.take(5)                                                     # show the top five lines



Skipped Datasets tutorial


 ___            _______          ____   ____  _____   ____         ____          _____ _____        _____         ____         ____  _______  _____
|    \     ^       |       ^    |    \ |    \   |    /     |    / /             |        |   |     |             /      \   / /         |    |      |\    /|
|     \   / \      |      / \   |    / |    |   |   /      |   /  \___          |        |   |     |             \___    \ /  \___      |    |      | \  / |
|     |  /___\     |     /___\  |---<  |____/   |   |      |--{       \         |----    |   |     |----             \    Y       \     |    |----  |  \/  |
|     / /     \    |    /     \ |    \ |   \    |   \      |   \       )        |        |   |     |                  )   |        )    |    |      |      |
|____/  |     |    |    |     | |____/ |    \ __|__  \____ |    \ ____/         |      __|__ |____ |_____        ____/    |   ____/     |    |_____ |      |

Databricks File System (DBFS)
ref: https://docs.databricks.com/data/databricks-file-system.html


display(dbutils.fs.ls("dbfs:/"))                            # show root files
dbutils.fs.mkdirs("/foobar/")                               # make a directory
dbutils.fs.put("/foobar/baz.txt", "Hello, World!")          # create a file and add test to it
dbutils.fs.rm("/foobar/baz.txt")                            # remove a file

shorthand commands:
%fs ls                                                      # List the DBFS root
%fs rm -r foobar                                            # Recursively remove the files under foobar
%fs put -f "/mnt/my-file" "Hello world!"                    # Overwrite the file "/mnt/my-file" with the string "Hello world!"


                   ____  _____   ____          ____   ___                                      ___     ____
|\    /|    ^     /        |    /             /      /   \  |\    /| |\    /|    ^    |\    | |    \  /
| \  / |   / \   /         |   /             /      /     \ | \  / | | \  / |   / \   | \   | |     \ \___
|  \/  |  /___\  |    _    |   |             |      |     | |  \/  | |  \/  |  /___\  |  \  | |     |     \
|      | /     \ \     /   |   \             \      \     / |      | |      | /     \ |   \ | |     /      )
|      | |     |  \___/  __|__  \____         \____  \___/  |      | |      | |     | |    \| |____/  ____/
Magic Commands:
%run                                                 # run another notebook
%run path/to/NotebookB $VarA="ValueA" $VarB="ValueB" # give parameters to a notebook
%sh                                                  # run shell code
%fs                                                  # File System commands
%md                                                  # include documentation

language switch commands:
%python                       # switch to using Python as the language
%r
%scala
%sql                          # switch to using SQL as the language


shortcuts:
shift+Enter                   # run a cell



_______  _____  ____           _____           ___           ___     ____
   |    |      |    \ |\    /|   |   |\    |  /   \  |      /   \   /      \   /
   |    |      |    | | \  / |   |   | \   | /     \ |     /     \ /        \ /
   |    |----  |____/ |  \/  |   |   |  \  | |     | |     |     | |    _    Y
   |    |      |   \  |      |   |   |   \ | \     / |     \     / \     /   |
   |    |_____ |    \ |      | __|__ |    \|  \___/  |____  \___/   \___/    |
Terminology:

cell
cluster
notebook
workspace

 ____   ____   _____  ___     _____  _____ _____          _____  ___                             ____  _____          ____         _____  ____
|    \ |    \ |      |    \  |      |        |   |\    | |      |    \          |     |    ^    |    \   |      ^    |    \ |     |      /
|    | |    | |      |     \ |      |        |   | \   | |      |     \         \     /   / \   |    |   |     / \   |    / |     |      \___
|____/ |____/ |----  |     | |----  |----    |   |  \  | |----  |     |          \   /   /___\  |____/   |    /___\  |---<  |     |----      \
|      |   \  |      |     / |      |        |   |   \ | |      |     /           \ /   /     \ |   \    |   /     \ |    \ |     |           )
|      |    \ |_____ |____/  |_____ |      __|__ |    \| |_____ |____/             V    |     | |    \ __|__ |     | |____/ |____ |_____ ____/
Predefined variables:

sc                             # Spark Context
spark                          # Spark Session
display(DF)                    # a visual way to see the DF dataframe

udf(FUNCTION)                  # a user defined function


Spark commands:
spark.createDataFrame(data,...)                   # create a DataFrame
spark.sql                                         # returns a DataFrame representing the result of the given query
spark.table(TABLE)                                # returns a DataFrame for a given table
spark.read.parquet("/path/of/file.parquet")       # returns a DataFrame for a given parquet file
spark.read.json("/path/of/file.json")             # returns a DataFrame for a given json file
spark.read.json(sc.parallelize([json_str]))       # returns a DataFrame for a given json string - https://stackoverflow.com/questions/49675860/pyspark-converting-json-string-to-dataframe
spark.udf.register(FUN_NAM, UDF_FUNCTION)         # register a function to be used in a SQL statement

 ___            _______                 _____  ____                    _____         _____                   ____ _______ _____   ___            ____
|    \     ^       |       ^           |      |    \    ^    |\    /| |             |      |     | |\    |  /        |      |    /   \  |\    | /
|     \   / \      |      / \          |      |    |   / \   | \  / | |             |      |     | | \   | /         |      |   /     \ | \   | \___   *
|     |  /___\     |     /___\         |----  |____/  /___\  |  \/  | |----         |----  |     | |  \  | |         |      |   |     | |  \  |     \
|     / /     \    |    /     \        |      |   \  /     \ |      | |             |      \     / |   \ | \         |      |   \     / |   \ |      ) *
|____/  |     |    |    |     |        |      |    \ |     | |      | |_____        |       \___/  |    \|  \____    |    __|__  \___/  |    \| ____/

Data frame functions:
printSchema()
count()
take(5)
write.mode("overwrite").parquet(outPath)                       # create a parquet file
write.mode("overwrite").saveAsTable(TABLE_NAME)                # This creates a global table, unlike the local view created by the DataFrame method createOrReplaceTempView
createOrReplaceTempView(TABLE)                                 # create a table that can be referenced from a SQL statement

distinct()

WithColumn("columnName", COLUMN DATA)
groupBy("columnName_N", "columnName_N+1").agg(...
select:
  df.select("event.eventDetail.identityInfo.supplyUniqueId")   # to flatten fields values in a struct


Action operations
-------------------
count()
collect()                   # get the entire contents of the dataframe, Cation don't do this on big data sets
show()                      # visually shows the first 20 records
take(NUM)                   # get NUM rows
first, head                 #
describe, summary           #

Narrow operations
------------------
read
select
filter/where
write
cast
union

Wide operations
--------------------
distinct
groupBy
sort/orderBy
join




Rows methods in Python
index
count
asDict
row.key
row["key"]
key in row




Widgets methods:
-----------------------------------------
# If you add a command to remove a widget, you cannot add a subsequent command to
# create a widget in the same cell. You must create the widget in another cell.
dbutils.widgets.text("X", "value_of_X")
dbutils.widgets.dropdown("X123", "1", [str(x) for x in range(1, 10)])
dbutils.widgets.get("X")
dbutils.widgets.remove("X")
dbutils.widgets.removeAll()
%run /path/to/notebook $X="10" $Y="1"                                  # pass argurments to set widget values. this e.g. will set the X & Y widgets






python libraries
from pyspark.sql.types import LongType, StringType, StructType, StructField

from pyspark.sql.functions import col
col(COLUMN)                    # gives you the column info for the context of the current dataframe




Setup CLI notes:
filename: .databricks.cfg
file contents:
host = URL
username = login
password = XXXXXXX



Installing Pyspark in windows:
--------------------------------------------------------------------------------
Install Java
Install Python
Install Apache Spark
Configure Spark installation
Install winutils
Configure log level of Spark


Tools
--------------------------------------------------------------------------------
https://datathirst.net/projects/dbfs-explorer/
