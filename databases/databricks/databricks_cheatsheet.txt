Apache Spark interfaces:
- Resilient Distributed Dataset (RDD)
    * available languages: Java, Python & Scala
- DataFrame
    * Superset of RDD
    * Unlike an RDD, data is organized into named columns
    * available languages: Java, Python & Scala
- DataSet
    * Combo of RDD & DataFrame
    * available languages: Java & Scala (no Python)


Python:
--------------------------------------------------------------------------------------------------------------
dbutils.fs.ls("/databricks-datasets/samples/docs/")                        # how to read a file location
textFile = spark.read.text("/databricks-datasets/samples/docs/README.md")  # how to load a file
textFile.count()                                                           # count the number lines in the file
textFile.first()                                                           # get the first line of the file
linesWithSpark = textFile.filter(textFile.value.contains("Spark"))         # filter the file with lines that have "Spark" in it
linesWithSpark.take(5)                                                     # show the top five lines

_____          ____  _______                     _____           ____          ____         ____   ____           ____                _____                         _____          ___      ___             ____
  |   |\    | /         |       ^    |     |       |   |\    |  /             |    \ \   / /      |    \    ^    |    \ |    /          |   |\    |        |      |   |   |\    | |    \   /   \  |      | /
  |   | \   | \___      |      / \   |     |       |   | \   | /              |    |  \ /  \___   |    |   / \   |    | |   /           |   | \   |        |      |   |   | \   | |     \ /     \ |      | \___
  |   |  \  |     \     |     /___\  |     |       |   |  \  | |    _         |____/   Y       \  |____/  /___\  |____/ |--{            |   |  \  |        |  /\  |   |   |  \  | |     | |     | |  /\  |     \
  |   |   \ |      )    |    /     \ |     |       |   |   \ | \     /        |        |        ) |      /     \ |   \  |   \           |   |   \ |        | /  \ |   |   |   \ | |     / \     / | /  \ |      )
__|__ |    \| ____/     |    |     | |____ |____ __|__ |    \|  \___/         |        |   ____/  |      |     | |    \ |    \        __|__ |    \|        |/    \| __|__ |    \| |____/   \___/  |/    \| ____/


Installing Pyspark in windows:
--------------------------------------------------------------------------------
Install Python
Install pyspark                        https://spark.apache.org/docs/latest/api/python/getting_started/index.html
Install Java
copy winutils.exe to C:\hadoop\bin     https://github.com/cdarlint/winutils
- Set the following environment variables
  Name            Path (e.g.)                                                Description
  --------------  --------------------------------------------------------   ------------------------------------
  JAVA_HOME       C:\Program Files\Java\jre1.8.0_271                         Java executable directory
  PYSPARK_PYTHON  C:\Users\brinkmep\AppData\Local\Programs\Python\Python39   Python executable directory
  HADOOP_HOME     C:\hadooop


  ____   ___                       ___                   ____   _____ _______          ____           ____   ___                                      ___     ____
 /      /   \  |\    /| |\    /|  /   \  |\    |        /      |         |    |     | |    \         /      /   \  |\    /| |\    /|    ^    |\    | |    \  /
/      /     \ | \  / | | \  / | /     \ | \   |        \___   |         |    |     | |    |        /      /     \ | \  / | | \  / |   / \   | \   | |     \ \___
|      |     | |  \/  | |  \/  | |     | |  \  |            \  |----     |    |     | |____/        |      |     | |  \/  | |  \/  |  /___\  |  \  | |     |     \
\      \     / |      | |      | \     / |   \ |             ) |         |    \     / |             \      \     / |      | |      | /     \ |   \ | |     /      )
 \____  \___/  |      | |      |  \___/  |    \|        ____/  |_____    |     \___/  |              \____  \___/  |      | |      | |     | |    \| |____/  ____/


Common setup commands:
---------------------------------------------
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

 ___            _______          ____   ____  _____   ____         ____          _____ _____        _____         ____         ____  _______  _____
|    \     ^       |       ^    |    \ |    \   |    /     |    / /             |        |   |     |             /      \   / /         |    |      |\    /|
|     \   / \      |      / \   |    / |    |   |   /      |   /  \___          |        |   |     |             \___    \ /  \___      |    |      | \  / |
|     |  /___\     |     /___\  |---<  |____/   |   |      |--{       \         |----    |   |     |----             \    Y       \     |    |----  |  \/  |
|     / /     \    |    /     \ |    \ |   \    |   \      |   \       )        |        |   |     |                  )   |        )    |    |      |      |
|____/  |     |    |    |     | |____/ |    \ __|__  \____ |    \ ____/         |      __|__ |____ |_____        ____/    |   ____/     |    |_____ |      |

Databricks File System (DBFS)
ref: https://docs.databricks.com/data/databricks-file-system.html


display(dbutils.fs.ls("dbfs:/"))                            # show root files
dbutils.fs.mkdirs("/foobar/")                               # make a directory
dbutils.fs.put("/foobar/baz.txt", "Hello, World!")          # create a file and add test to it
dbutils.fs.rm("/foobar/baz.txt")                            # remove a file
dbutils.notebook.run(<PATH>, <TIME_OUT_IN_SEC>)             # run another notebook in an isolated environment

shorthand commands:
%fs ls                                                      # List the DBFS root
%fs mkdirs <DIR>                                            # make a directory (note the command has an "s" at the end unlike the linux command)
#fs cp <SRC> <DEST>                                         # copy a file
%fs mv                                                      # move/rename a file
%fs rm -r foobar                                            # Recursively remove the files under foobar
%fs put -f "/mnt/my-file" "Hello world!"                    # Overwrite the file "/mnt/my-file" with the string "Hello world!"
%fs head dbfs:/path/to/file                                 # kind of like the linux "cat" command

FileStore
---------------------------------------------------
e.g. URL way to download a file from the FileStore:
https://dataos-dev.cloud.databricks.com/files/shared_uploads/paul.brinkmeyer@hp.com/schema.json

                   ____  _____   ____          ____   ___                                      ___     ____
|\    /|    ^     /        |    /             /      /   \  |\    /| |\    /|    ^    |\    | |    \  /
| \  / |   / \   /         |   /             /      /     \ | \  / | | \  / |   / \   | \   | |     \ \___
|  \/  |  /___\  |    _    |   |             |      |     | |  \/  | |  \/  |  /___\  |  \  | |     |     \
|      | /     \ \     /   |   \             \      \     / |      | |      | /     \ |   \ | |     /      )
|      | |     |  \___/  __|__  \____         \____  \___/  |      | |      | |     | |    \| |____/  ____/
Magic Commands:
%run                                                 # run another notebook using the current environment unlike dbutils.notebook.run() which isolates its environment
%run path/to/NotebookB $VarA="ValueA" $VarB="ValueB" # give parameters to a notebook
%sh                                                  # run shell code
%fs                                                  # File System commands
%md                                                  # include documentation

language switch commands:
%python                       # switch to using Python as the language
%r
%scala
%sql                          # switch to using SQL as the language


shortcuts:
shift+Enter                   # run a cell



_______  _____  ____           _____           ___           ___     ____
   |    |      |    \ |\    /|   |   |\    |  /   \  |      /   \   /      \   /
   |    |      |    | | \  / |   |   | \   | /     \ |     /     \ /        \ /
   |    |----  |____/ |  \/  |   |   |  \  | |     | |     |     | |    _    Y
   |    |      |   \  |      |   |   |   \ | \     / |     \     / \     /   |
   |    |_____ |    \ |      | __|__ |    \|  \___/  |____  \___/   \___/    |
Terminology:

cell
cluster
notebook
workspace

 ____   ____   _____  ___     _____  _____ _____          _____  ___                             ____  _____          ____         _____  ____
|    \ |    \ |      |    \  |      |        |   |\    | |      |    \          |     |    ^    |    \   |      ^    |    \ |     |      /
|    | |    | |      |     \ |      |        |   | \   | |      |     \         \     /   / \   |    |   |     / \   |    / |     |      \___
|____/ |____/ |----  |     | |----  |----    |   |  \  | |----  |     |          \   /   /___\  |____/   |    /___\  |---<  |     |----      \
|      |   \  |      |     / |      |        |   |   \ | |      |     /           \ /   /     \ |   \    |   /     \ |    \ |     |           )
|      |    \ |_____ |____/  |_____ |      __|__ |    \| |_____ |____/             V    |     | |    \ __|__ |     | |____/ |____ |_____ ____/
Predefined variables:

sc                             # Spark Context
spark                          # Spark Session
display(DF)                    # a visual way to see the DF data
udf(FUNCTION)                  # a user defined function

 ____   ____           ____                  ____   ___                                      ___     ____
/      |    \    ^    |    \ |    /         /      /   \  |\    /| |\    /|    ^    |\    | |    \  /
\___   |    |   / \   |    | |   /         /      /     \ | \  / | | \  / |   / \   | \   | |     \ \___
    \  |____/  /___\  |____/ |--{          |      |     | |  \/  | |  \/  |  /___\  |  \  | |     |     \
     ) |      /     \ |   \  |   \         \      \     / |      | |      | /     \ |   \ | |     /      )
____/  |      |     | |    \ |    \         \____  \___/  |      | |      | |     | |    \| |____/  ____/

Spark commands:
spark.createDataFrame(data,...)                    # create a DataFrame
spark.sql()                                        # returns a DataFrame representing the result of the given query
spark.table(TABLE)                                 # returns a DataFrame for a given table
spark.read.csv("/path/of/file.csv", header="true") # returns a DataFrame for a given csv file
spark.read.parquet("/path/of/file.parquet")        # returns a DataFrame for a given parquet file
spark.read.json("/path/of/file.json")              # returns a DataFrame for a given json file
spark.read.json(sc.parallelize([json_str]))        # returns a DataFrame for a given json string - https://stackoverflow.com/questions/49675860/pyspark-converting-json-string-to-dataframe
spark.read.text(/path/of/file.txt")                # returns a DataFrame for a given file where each line is a row
spark.udf.register(FUN_NAM, UDF_FUNCTION)          # register a function to be used in a SQL statement


 ___            _______                 _____  ____                    _____         _____                   ____ _______ _____   ___            ____
|    \     ^       |       ^           |      |    \    ^    |\    /| |             |      |     | |\    |  /        |      |    /   \  |\    | /
|     \   / \      |      / \          |      |    |   / \   | \  / | |             |      |     | | \   | /         |      |   /     \ | \   | \___
|     |  /___\     |     /___\         |----  |____/  /___\  |  \/  | |----         |----  |     | |  \  | |         |      |   |     | |  \  |     \
|     / /     \    |    /     \        |      |   \  /     \ |      | |             |      \     / |   \ | \         |      |   \     / |   \ |      )
|____/  |     |    |    |     |        |      |    \ |     | |      | |_____        |       \___/  |    \|  \____    |    __|__  \___/  |    \| ____/

Data frame functions:

Link:
https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html?highlight=dataframe#pyspark.sql.DataFrame


printSchema()
cache()                                                        # Cache data for faster reuse
count()                                                        # the number of records in the data frame
distinct()                                                     # show only one instance of a unique row
dropna()                                                       # drop rows with missing values
filter()                                                       # e.g. df.filter(df.field_name == "value_to_filter_for")
filter(df.col_name == "some_value")                            # filter on col_name with values that are equal to "some_value"
filter(df.col_name != "some_value")                            # filter on col_name with values that are not equal to "some_value"
filter(df.col_name.isin(list_of_values))                       # filter on col_name with any value in list_of_values
filter(df.col_name.isNull())                                   # filter on col_name with values that are null
filter(df.col_name.isNotNull())                                # filter on col_name with values that are not null
limit(num)                                                     # Limits the result count to the number specified. e.g. limit(0) will give you an empty data frame but will keep the schema intact
orderBy(field, ascending=False)                                # sort by field in descending order
select(...)                                                    # show only the given columns from the DF
selectExpr()
take(num)                                                      # Returns the first num rows as a list of Row. It is different from limit() in that it doesn't return a dataframe. It returns the actual data.
withColumn("columnName", COLUMN DATA)                          # update column data (does not work with nested structs)
write.mode("overwrite").parquet(outPath)                       # create a parquet file
write.mode("overwrite").saveAsTable(TABLE_NAME)                # This creates a global table, unlike the local view created by the DataFrame method createOrReplaceTempView
write.format("delta").mode("overwrite").save(outPath)          # create a delta table and remove the previous one
createOrReplaceTempView(TABLE)                                 # create a table that can be referenced from a SQL statement



groupBy("columnName_N", "columnName_N+1").agg(...
select:
  df.select("event.eventDetail.identityInfo.supplyUniqueId")   # to flatten fields values in a struct


Action operations
-------------------
count()
collect()                   # get the entire contents of the dataframe, Cation don't do this on big data sets
show()                      # visually shows the first 20 records
take(NUM)                   # get NUM rows
first, head                 #
describe, summary           #

Narrow operations
------------------
read
select
filter/where
write
cast
union

Wide operations
--------------------
distinct
groupBy
sort/orderBy
join


Schema:
------------------------------
df = spark.createDataFrame(df.rdd, new_schema)      # useful way to update the schema ref: https://chih-ling-hsu.github.io/2017/03/28/how-to-change-schema-of-a-spark-sql-dataframe




Rows methods in Python
index
count
asDict
row.key
row["key"]
key in row




Widgets methods:
-----------------------------------------
# If you add a command to remove a widget, you cannot add a subsequent command to
# create a widget in the same cell. You must create the widget in another cell.
dbutils.widgets.help("text")                                           # will give info about the text function, text can be any other function
dbutils.widgets.text("X", "value_of_X_on_first_init")                  # will created the widget at the top of the notebook
dbutils.widgets.dropdown("X123", "1", [str(x) for x in range(1, 10)])
dbutils.widgets.combobox(...)
dbutils.widgets.multiselect(...)
dbutils.widgets.get("X")                                               # get the value of a widget. will error if not available
dbutils.widgets.remove("X")                                            # will remove the widget from the notebook
dbutils.widgets.removeAll()
%run /path/to/notebook $X="10" $Y="1"                                  # pass argurments to set widget values. this e.g. will set the X & Y widgets





python libraries
from pyspark.sql.types import LongType, StringType, StructType, StructField

from pyspark.sql.functions import col
col(COLUMN)                    # gives you the column info for the context of the current dataframe




Setup CLI notes:
filename: .databricks.cfg
file contents:
host = URL
username = login
password = XXXXXXX






Tools
--------------------------------------------------------------------------------
https://datathirst.net/projects/dbfs-explorer/





Scheam notes
-----------------------------------
df.schema                           # 
df.printSchema()                    # nice visual way to see it
df.schema.jsonValue()               # will dump the schema in a JSON format
